{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818557f2-eeb5-40c3-920c-f014a5af81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from model import *\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00304907-e0f0-487b-84f4-11a287c467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정하기\n",
    "import random\n",
    "import os\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "seed_everything()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec118259",
   "metadata": {},
   "source": [
    "### Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208e2a1c-288b-4615-b49a-52faa4ef34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "path = '../Transformer'\n",
    "\n",
    "def make_processed_data(path):\n",
    "    '''\n",
    "    기업명과 사업자번호를 key로 사용하려고 했으나, 하나의 기업명에 대해 여러개의 사업자번호가 맵핑되어있는 경우가 발생\n",
    "    따라서 하나의 기업을 의미하면, 하나의 key로 맵핑될 수 있도록 처리\n",
    "    '''\n",
    "    data_path = path + '/data'\n",
    "    df_biz = pd.read_csv(data_path + '/patent_companies.tsv', sep = '\\t', parse_dates=['출원일자'])\n",
    "    number2embedding = dict(zip(np.load(data_path + '/number.npy', allow_pickle = True), np.load(data_path + '/embedding.npy')))\n",
    "    df_biz['embedding'] = df_biz['출원번호'].map(number2embedding)\n",
    "    df_biz['사업자번호'] = df_biz['사업자번호'].apply(lambda x: re.sub('-','',x))\n",
    "    biz2common = pd.read_table('./biz2common.tsv',dtype={'사업자번호':'object','index':'object'})\n",
    "    biznum2idx = dict(zip(biz2common['사업자번호'],biz2common['index']))\n",
    "    df_biz['key'] = df_biz['사업자번호'].map(biznum2idx)\n",
    "    df_biz['key'] = df_biz['key'].astype(object)\n",
    "    \n",
    "    df_itr = pd.read_csv('./mna_checking_patent.csv', parse_dates=['contract_date'])\n",
    "    df_itr =  df_itr[df_itr['contract_date'].notna()]\n",
    "    return df_biz, df_itr\n",
    "\n",
    "df, df_itr = make_processed_data(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80eee814",
   "metadata": {},
   "source": [
    "### interaction이 발생한 회사들 기준으로 train, test corp split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1317a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# target 기업 중 interaction 이전에 특허가 없는 경우 -> index=468\n",
    "df_itr = df_itr.drop(index=468)\n",
    "drop_key = ['12526', '96244', '96244', '111569', '94218', '69689', '66197', '96244']\n",
    "drop_key = set(map(lambda x:int(x),drop_key))\n",
    "for key in drop_key:\n",
    "    idx = df_itr[(df_itr['corp_key'].isin(drop_key)) | (df_itr['partner_key'].isin(drop_key))].index\n",
    "    df_itr = df_itr.drop(index=idx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7f000f-b9e9-41ee-86d3-9bb6ee41b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corp(df_patent, df_itr):\n",
    "    '''\n",
    "    의뢰기업, 대상기업의 최근 출원일자 기준으로 구분\n",
    "    '''\n",
    "    key_dates = df_patent.groupby('key')['출원일자'].max()\n",
    "\n",
    "    df_itr_copy = df_itr.copy()\n",
    "    df_itr_copy['corp_key'] = df_itr_copy['corp_key'].astype(str)\n",
    "    df_itr_copy['partner_key'] = df_itr_copy['partner_key'].astype(str)\n",
    "    df_itr_copy['client_key'] = df_itr_copy['corp_key']\n",
    "    df_itr_copy['target_key'] = df_itr_copy['partner_key']\n",
    "\n",
    "    key_dates_df = key_dates.to_frame('max_date').reset_index()\n",
    "    df_itr_copy = df_itr_copy.merge(key_dates_df, left_on='corp_key', right_on='key')\n",
    "    df_itr_copy = df_itr_copy.merge(key_dates_df.rename(columns={'max_date': 'partner_max_date'}), left_on='partner_key', right_on='key')\n",
    "\n",
    "    client_mask = df_itr_copy['max_date'] >= df_itr_copy['partner_max_date']\n",
    "    df_itr_copy.loc[client_mask, 'client_key'] = df_itr_copy.loc[client_mask, 'corp_key'].astype(str)\n",
    "    df_itr_copy.loc[~client_mask, 'client_key'] = df_itr_copy.loc[~client_mask, 'partner_key'].astype(str)\n",
    "    df_itr_copy.loc[client_mask, 'target_key'] = df_itr_copy.loc[client_mask, 'partner_key'].astype(str)\n",
    "    df_itr_copy.loc[~client_mask, 'target_key'] = df_itr_copy.loc[~client_mask, 'corp_key'].astype(str)\n",
    "    \n",
    "#     client_mask = df_itr_copy['max_date'] >= df_itr_copy['partner_max_date']\n",
    "#     df_itr_copy['client_key'] = df_itr_copy['corp_key'].astype(str)\n",
    "#     df_itr_copy['target_key'] = df_itr_copy['partner_key'].astype(str)\n",
    "#     df_itr_copy.loc[~client_mask, ['client_key', 'target_key']] = df_itr_copy.loc[~client_mask, ['partner_key', 'corp_key']].astype(str)\n",
    "    return df_itr_copy\n",
    "\n",
    "\n",
    "def filter_by_contract_date(df_patent, df_itr, key):\n",
    "    '''\n",
    "    interaction 발생일자 이전의 데이터 추출\n",
    "    '''\n",
    "    filtered_df_patent = pd.DataFrame()\n",
    "    not_exist_patent = []\n",
    "    for index, row in df_itr.iterrows():\n",
    "        contract_date = row['contract_date']\n",
    "        if key == 'client_key':\n",
    "            corp_key = row['client_key']\n",
    "            target_key = row['target_key']\n",
    "        else:\n",
    "            corp_key = row['target_key']\n",
    "            target_key = row['client_key']\n",
    "        filtered_data = df_patent[(df_patent['출원일자'] <= contract_date) & (df_patent['key'] == corp_key)].sort_values(by='출원일자', ascending=False)\n",
    "        if filtered_data.empty:\n",
    "            not_exist_patent.append((corp_key, target_key))\n",
    "        else:\n",
    "            filtered_data['identifier'] = str(index)\n",
    "            filtered_df_patent = pd.concat([filtered_df_patent, filtered_data])\n",
    "    return filtered_df_patent, not_exist_patent\n",
    "\n",
    "def create_by_contract_date(df_patent, df_itr, key_pairs, not_exist_patent):\n",
    "    result = []\n",
    "\n",
    "    target_keys = key_pairs['target_key']\n",
    "    df_patent_filtered = df_patent[df_patent['key'].isin(target_keys)]\n",
    "    df_patent_sorted = df_patent_filtered.sort_values(by='출원일자', ascending=False)\n",
    "\n",
    "    for _, key_pair in key_pairs.iterrows():\n",
    "        client_key = key_pair['client_key']\n",
    "        target_key = key_pair['target_key']\n",
    "        if ((client_key, target_key) in not_exist_patent) or ((target_key, client_key) in not_exist_patent):\n",
    "            continue\n",
    "        filtered_data = df_patent_sorted[df_patent_sorted['key'] == target_key]\n",
    "        if not filtered_data.empty:\n",
    "            last_data = filtered_data.iloc[0]\n",
    "            result.append(last_data['embedding'])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def interaction_patent(df_patent, df_itr):\n",
    "    '''\n",
    "    client_key : 의뢰기업 key\n",
    "    target_key : 대상기업 key\n",
    "    '''\n",
    "    df_itr = df_itr[df_itr['corp_patent'] & df_itr['partner_patent']]\n",
    "    df_itr = compare_corp(df_patent, df_itr)\n",
    "    client_key = df_itr['client_key']\n",
    "    target_key = df_itr['target_key']\n",
    "    concat_key = pd.concat([client_key,target_key])\n",
    "    key_pairs = pd.concat([client_key, target_key], axis=1)\n",
    "    \n",
    "    df_train = df_patent[~df_patent['key'].isin(concat_key)]\n",
    "    \n",
    "    temp = df_patent[df_patent['key'].isin(client_key)]\n",
    "    df_test, not_exist_patent = filter_by_contract_date(temp, df_itr, 'client_key')\n",
    "    temp_patent, _ = filter_by_contract_date(df_patent, df_itr, 'target_key')\n",
    "    patent_array = create_by_contract_date(temp_patent, df_itr, key_pairs, not_exist_patent)\n",
    "    \n",
    "    return df_train, df_test, patent_array\n",
    "    \n",
    "df_train, df_test, patent_array = interaction_patent(df, df_itr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8857819-abb5-437e-a0c4-063663ebfa12",
   "metadata": {},
   "source": [
    "### develop 단계에서만 필요 -> file: dataset / model / train / annoy / inference / measure   .py로 작업 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97208fe4-6db8-4c89-9885-a7e35634b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def preprocess_dataset(df_org,mode):\n",
    "    '''\n",
    "    df_above: 기업에서 낸 특허가 2개 이상인 경우\n",
    "    df_under : 기업에서 낸 특허가 2개 이하인 경우, 기업에서 낸 마지막 특허를 바로 ANN 알고리즘으로 찾기 \n",
    "    '''\n",
    "    df = copy.deepcopy(df_org)\n",
    "    # df = df[df['출원일자'].notna()]\n",
    "    col = 'key' if mode == 'train' else 'identifier'\n",
    "    df_above,df_under = ck_above_min(df,col)\n",
    "    df_under = df_under.drop_duplicates(subset=col,keep='last')\n",
    "    count_table, count_dict,key2idx, idx2key = make_idx_map(df_above,col)\n",
    "    df_above = pd.merge(df_above,count_table[[col,'index']],on=col)\n",
    "    df_above = sort_by_filing_date(df_above,col)\n",
    "    return df_above,df_under,count_table,count_dict,key2idx,idx2key\n",
    "\n",
    "# sequential성을 따지기 위해서 회사가 보유한 특허가 3개 이상은 되어야 함. -> 그래야 sequential한 모델링 가능 \n",
    "def ck_above_min(df,col):\n",
    "    temp = df[col].value_counts().reset_index()\n",
    "    temp['above_min'] = temp['count'] >= 2\n",
    "    df_count = pd.merge(df,temp[[col,'above_min']],on=col,how='inner')\n",
    "    df_above = df_count[df_count['above_min']==True].drop('above_min',axis=1)\n",
    "    df_under = df_count[df_count['above_min']==False].drop('above_min',axis=1)\n",
    "    return df_above,df_under\n",
    "\n",
    "def make_idx_map(df,col):\n",
    "    count_table = df[col].value_counts().reset_index().reset_index()\n",
    "    count_dict = dict(zip(count_table['index'],count_table['count']))\n",
    "    key2idx = dict(zip(count_table[col],count_table['index']))\n",
    "    idx2key = dict(zip(count_table['index'],count_table[col]))\n",
    "    return count_table, count_dict,key2idx, idx2key\n",
    "    \n",
    "def sort_by_filing_date(df,col):\n",
    "    return df.groupby(col).apply(lambda x:x.sort_values(by='출원일자'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe23618-4582-4d37-8d8d-4262c522815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biz,df_under,count_table, count_dict, key2idx,idx2key = preprocess_dataset(df_train,'train')\n",
    "df_biz_test,df_under_test,count_table_test,count_dict_test,key2idx_test,idx2key_test = preprocess_dataset(df_test,'test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81828b36",
   "metadata": {},
   "source": [
    "### Sequential하게 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72dcf2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "def make_args(count_dict):\n",
    "    import argparse\n",
    "\n",
    "    # default args\n",
    "    args = argparse.Namespace(\n",
    "        batch_size=16,\n",
    "        d_embed=768,\n",
    "        max_len = 1024,\n",
    "        initializer_range = 0.02,\n",
    "        num_epochs = 5,\n",
    "        num_heads = 4,\n",
    "        num_layers = 2,\n",
    "        dropout_rate = 0.2,\n",
    "        lr = 0.005,\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        num_items = len(count_dict)\n",
    "        annoy_path = './model_pt/patent_ann.annoy',\n",
    "        n_trees = 20,\n",
    "        n = 100\n",
    "    )\n",
    "    return args\n",
    "def make_seq_main(args,df_biz,count_dict):\n",
    "    def make_corp_seq(args,df):\n",
    "        embedding_dict = {} \n",
    "        for idx,cnt in count_dict.items():\n",
    "            seq = cnt\n",
    "            part_sequence = []\n",
    "            if seq >= args.max_len:\n",
    "                corp_seq = df[df['index']== idx]['embedding'][-(args.max_len + 2) : ]\n",
    "            else:\n",
    "                corp_seq = df[df['index']== idx]['embedding']\n",
    "            for i in range(len(corp_seq)):\n",
    "                part_sequence.append(corp_seq[i])\n",
    "            embedding_matrix = np.array(part_sequence).reshape(-1,args.d_embed)\n",
    "            embedding_dict[idx] = embedding_matrix\n",
    "        return embedding_dict\n",
    "    corp_seq = make_corp_seq(args,df_biz)\n",
    "    return corp_seq\n",
    "corp_seq = make_seq_main(make_args(count_dict),df_biz,count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5395d4-ee93-41e3-8999-4e605348925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = make_args(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac88e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = Dataset(args, corp_seq, data_type=\"train\")\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler, batch_size=args.batch_size\n",
    ")\n",
    "\n",
    "eval_dataset = Dataset(args, corp_seq, data_type=\"valid\")\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, sampler=eval_sampler, batch_size=args.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477b77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f28ff695",
   "metadata": {},
   "source": [
    "### 불러와서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45a01b69-243e-4257-abcb-b3b3b67115ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math\n",
    "class TransformerLoss(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TransformerLoss, self).__init__()\n",
    "        self.hidden_size = args.d_embed\n",
    "        self.max_seq_length = args.max_len\n",
    "\n",
    "    def forward(self, output, target, mask):\n",
    "        \n",
    "        output_emb = output.view(-1, self.hidden_size)\n",
    "        target_emb = target.view(-1, self.hidden_size)\n",
    "\n",
    "        output_norm = torch.norm(output_emb, dim=-1)\n",
    "        target_norm = torch.norm(target_emb, dim=-1)\n",
    "        \n",
    "        output_norm = output_norm.clamp(min=1e-10)\n",
    "        target_norm = target_norm.clamp(min=1e-10)\n",
    "        \n",
    "        logits = torch.sum(target_emb * output_emb, -1) / (output_norm * target_norm)\n",
    "        angular_distance = 1 - torch.arccos(logits) / torch.tensor(math.pi)\n",
    "        istarget = (mask > 0).view(mask.size(0) * self.max_seq_length).float()\n",
    "        # print(((logits) + 1e-24) * istarget)\n",
    "        loss = torch.sum(-torch.log((logits) + 1e-24) * istarget) / torch.sum(istarget)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4283c2e2-4e4e-4898-af91-9e87be62abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        args,\n",
    "    ):\n",
    "\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        self.lr = args.lr\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.model = model\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        \n",
    "        self.criterion = TransformerLoss(self.args)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_dataloader)\n",
    "\n",
    "    def valid(self, epoch):\n",
    "        return self.iteration(epoch, self.eval_dataloader, mode=\"valid\")\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        torch.save(self.model.cpu().state_dict(), file_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load(self, file_name):\n",
    "        self.model.load_state_dict(torch.load(file_name))\n",
    "        \n",
    "    def iteration(self, epoch, dataloader, mode=\"train\"):\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        \n",
    "        tbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for corp_id, mask, input_seq, target_pos, _ in tbar: ##\n",
    "                mask = mask.to(self.device)\n",
    "                input_seq, target_pos = input_seq.to(self.device), target_pos.to(self.device)\n",
    "                output = self.model(input_seq, mask=mask)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(output, target_pos, mask) ##\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Train Loss: {avg_loss:.4f}\")\n",
    "            self.save(f'./model_pt/model_state_dict_epoch{epoch}.pt') \n",
    "\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for corp_id, mask, input_seq, target_pos, _ in tbar: ##\n",
    "                mask = mask.to(self.device)\n",
    "                input_seq, target_pos = input_seq.to(self.device), target_pos.to(self.device)\n",
    "                output = self.model(input_seq, mask=mask)\n",
    "                loss = self.criterion(output, target_pos, mask) ##\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Valid Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "# setting\n",
    "args = make_args(count_dict)\n",
    "model = setting_model(args)\n",
    "\n",
    "trainer = Trainer(model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        args)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    trainer.train(epoch)\n",
    "    trainer.valid(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e9280d5",
   "metadata": {},
   "source": [
    "### test dataset으로 넘어온 의뢰기업에 대해서\n",
    "1. make_corp_seq해서 sequential data로 만들기\n",
    "2. Dataset에 type = 'submission' 형태로 넣어서 만들기 -> answer로 넘어올 것 \n",
    "3. 모델에 inference해서 나온 벡터 준비 완료\n",
    "### test dataset으로 넘어온 대상 기업에 대해서\n",
    "1. 마지막 embedding vector만 가져오기\n",
    "### cosine 유사도 구하는 부분\n",
    "1. 모델에 inference해서 나온 벡터와 마지막 embedding vector간의 유사도 구하기\n",
    "2. measure 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20b0ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Measure:\n",
    "    '''\n",
    "    Model에 대한 Measure를 측정하는 부분\n",
    "    '''\n",
    "    def __init__(self,args,model,df_biz_test,target):\n",
    "        self.args = args\n",
    "        self.corp_seq = make_seq_main(self.args,df_biz_test,count_dict_test)\n",
    "        self.model = model\n",
    "        self.target = target\n",
    "        self.test_dataset = Dataset(self.args, self.corp_seq, data_type=\"submission\")\n",
    "        self.test_sampler = SequentialSampler(self.test_dataset)\n",
    "        self.test_dataloader = DataLoader(\n",
    "        self.test_dataset, sampler=self.test_sampler, batch_size=len(self.corp_seq)\n",
    "        )\n",
    "    \n",
    "    def calculate_similarity(self,output,target):\n",
    "        cosine_sim = np.sum(output * target, -1)/(np.linalg.norm(output,axis=-1) * np.linalg.norm(target,axis=-1))\n",
    "        angular_distance = (1 - np.arccos(cosine_sim) / math.pi)\n",
    "        return angular_distance\n",
    "    \n",
    "    def inference(self):\n",
    "        device = torch.device(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            tbar = tqdm(self.test_dataloader)\n",
    "            for corp_id, mask, input_seq, target_pos, _ in tbar: ##\n",
    "                mask = mask.to(device)\n",
    "                input_seq, target_pos = input_seq.to(device), target_pos.to(device)\n",
    "                output = self.model(input_seq, mask=mask)\n",
    "                output = output[:, -1, :].cpu().data.numpy()\n",
    "                total_sim = np.sum(self.calculate_similarity(output,self.target),-1) / len(self.corp_seq)\n",
    "\n",
    "        return total_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea79ccb6-c3ec-4971-9eec-f4c6526a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_array = np.array(patent_array)\n",
    "def infer(epoch):\n",
    "    model = setting_model(args)\n",
    "    model.load_state_dict(torch.load(f'./model_pt/model_state_dict_epoch{epoch}.pt'))\n",
    "    measure = Measure(make_args(count_dict_test),model,df_biz_test,patent_array)\n",
    "    return measure.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b77fc99-5eaf-4546-803a-7d14a0f712ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.716661807586721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7178344216668051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7148797913817913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708952355954603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7147698385364968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7131158278434557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7134164706342006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7117098583040509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142746921095051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(infer(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad716c87",
   "metadata": {},
   "source": [
    "### ANNOY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bf59339-e734-4113-b036-a91f6f85d402",
   "metadata": {},
   "source": [
    "annoy의 경우에는 index와 vector 정보만으로 동작하므로 각 vector에 대한 부가정보는 따로 저장을 해두어야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b2525942-a70b-4fd3-9dc5-6903eec474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_annoy(df_biz,args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_path = '/opt/ml/transformer_pytorch_custom/model_pt/model_state_dict_epoch0.pt'\n",
    "key = '100494'\n",
    "transformer = setting_model(make_args(count_dict))\n",
    "transformer.load_state_dict(torch.load(pt_path))\n",
    "annoy_index = AnnoyIndex(args.d_embed, 'angular')\n",
    "annoy_index.load(args.annoy_path)\n",
    "inference = Inference(args,corp_seq,key,transformer,annoy_index,count_dict,key2idx)\n",
    "# score_dict = inference.scoring(df_biz)\n",
    "inference_return = inference.find_candidates(df_biz)\n",
    "df_inference = inference.filtering(inference_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

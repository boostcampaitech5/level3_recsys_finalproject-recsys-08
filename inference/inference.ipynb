{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee025912-38c2-48d1-95cc-ed43656939fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import socket\n",
    "import json\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "from datetime import timedelta\n",
    "from google.cloud import storage\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b1efdc-1869-40d1-8c12-347892e907bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argparse:\n",
    "    pass\n",
    "\n",
    "def seed_everything(args):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "    return None\n",
    "\n",
    "def init_gcs(args):\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = args.server_data_path + args.key_name\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(args.bucket_name)\n",
    "\n",
    "    return bucket\n",
    "\n",
    "def get_url_from_gcs(args, path, file_name, expiration = 1):\n",
    "    # file_name = unicodedata.normalize('NFD', '아트웨어')\n",
    "    # path = path + file_name\n",
    "    return args.bucket.blob(path + file_name).generate_signed_url(version=\"v4\", expiration = timedelta(expiration), method=\"GET\") \n",
    "\n",
    "# 지우, 예림\n",
    "\n",
    "\n",
    "def make_data_frame(df_biz, number, embedding, biz2common):\n",
    "    number2embedding = dict(zip(number, embedding))\n",
    "    df_biz['embedding'] = df_biz['출원번호'].map(number2embedding)\n",
    "    df_biz['사업자번호'] = df_biz['사업자번호'].apply(lambda x: re.sub('-','',x))\n",
    "    \n",
    "    biznum2idx = dict(zip(biz2common['사업자번호'],biz2common['index']))\n",
    "    df_biz['key'] = df_biz['사업자번호'].map(biznum2idx)\n",
    "    df_biz['key'] = df_biz['key'].astype(object)\n",
    "    \n",
    "    return df_biz\n",
    "\n",
    "def preprocess_dataset(df_org,mode):\n",
    "    '''\n",
    "    df_above: 기업에서 낸 특허가 2개 이상인 경우\n",
    "    df_under : 기업에서 낸 특허가 2개 이하인 경우, 기업에서 낸 마지막 특허를 바로 ANN 알고리즘으로 찾기 \n",
    "    '''\n",
    "    df = copy.deepcopy(df_org)\n",
    "    col = 'key' if mode == 'train' else 'identifier'\n",
    "    df_above,df_under = ck_above_min(df,col)\n",
    "    df_under = df_under.drop_duplicates(subset=col,keep='last')\n",
    "    count_table, count_dict,corp2idx, idx2corp = make_idx_map(df_above,col)\n",
    "    df_above = pd.merge(df_above,count_table[[col,'index']],on=col)\n",
    "    df_above = sort_by_filing_date(df_above,col)\n",
    "    return df_above,df_under,count_table,count_dict,corp2idx,idx2corp\n",
    "\n",
    "def ck_above_min(df,col):\n",
    "    temp = df[col].value_counts().reset_index()\n",
    "    temp['above_min'] = temp['count'] >= 2\n",
    "    df_count = pd.merge(df,temp[[col,'above_min']],on=col,how='inner')\n",
    "    df_above = df_count[df_count['above_min']==True].drop('above_min',axis=1)\n",
    "    df_under = df_count[df_count['above_min']==False].drop('above_min',axis=1)\n",
    "    return df_above,df_under\n",
    "\n",
    "def make_idx_map(df,col):\n",
    "    count_table = df[col].value_counts().reset_index().reset_index()\n",
    "    count_dict = dict(zip(count_table['index'],count_table['count']))\n",
    "    key2idx = dict(zip(count_table[col],count_table['index']))\n",
    "    idx2key = dict(zip(count_table['index'],count_table[col]))\n",
    "    return count_table, count_dict,key2idx, idx2key\n",
    "    \n",
    "def sort_by_filing_date(df,col):\n",
    "    return df.groupby(col).apply(lambda x:x.sort_values(by='출원일자'))\n",
    "\n",
    "def make_seq_main(args, df, count_dict):\n",
    "    def make_corp_seq(args,df):\n",
    "        embedding_dict = {} \n",
    "        for idx,cnt in count_dict.items():\n",
    "            seq = cnt\n",
    "            part_sequence = []\n",
    "            if seq >= args.max_len:\n",
    "                corp_seq = df[df['index']== idx]['embedding'][-(args.max_len + 2) : ]\n",
    "            else:\n",
    "                corp_seq = df[df['index']== idx]['embedding']\n",
    "            \n",
    "            for i in range(len(corp_seq)):\n",
    "                part_sequence.append(corp_seq[i])\n",
    "            \n",
    "            embedding_matrix = np.array(part_sequence).reshape(-1,args.d_embed)\n",
    "            embedding_dict[idx] = embedding_matrix\n",
    "        \n",
    "        return embedding_dict\n",
    "    \n",
    "    corp_seq = make_corp_seq(args, df)\n",
    "    \n",
    "    return corp_seq\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, args, corp_seq, data_type=\"train\"):\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        self.corp_seq = corp_seq\n",
    "        self.data_type = data_type\n",
    "        self.max_len = args.max_len\n",
    "        self.d_embed = args.d_embed\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corp_seq)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        corp_id = index\n",
    "        patents = self.corp_seq[index] # index번째 기업의 sequence \n",
    "\n",
    "        if self.data_type == \"train\":\n",
    "            input_ids = patents[:-2]\n",
    "            target_pos = patents[1:-1]\n",
    "            answer = [0] # no use\n",
    "\n",
    "        elif self.data_type == \"valid\":\n",
    "            input_ids = patents[:-1]\n",
    "            target_pos = patents[1:]\n",
    "            # answer = [patents[-1]]\n",
    "            answer = np.array([patents[-1]])\n",
    "        else:\n",
    "            input_ids = patents[:]\n",
    "            target_pos = patents[:]  # will not be used\n",
    "            answer = []\n",
    "        \n",
    "        mask = np.ones(self.max_len)\n",
    "        pad_len = self.max_len - len(input_ids)\n",
    "        if pad_len > 0 :\n",
    "            pad = np.zeros((pad_len,self.d_embed))\n",
    "            input_ids = np.concatenate((pad,input_ids),axis=0)\n",
    "            target_pos = np.concatenate((pad,target_pos),axis=0)\n",
    "\n",
    "            mask[:pad_len] = 0\n",
    "            \n",
    "        input_ids = input_ids[-self.max_len :]\n",
    "        target_pos = target_pos[-self.max_len :]\n",
    "\n",
    "        assert len(input_ids) == self.max_len\n",
    "        assert len(target_pos) == self.max_len\n",
    "\n",
    "        cur_tensors = (\n",
    "            torch.tensor(corp_id, device=self.device),  # user_id for testing\n",
    "            torch.tensor(mask, device=self.device),  # user_id for testing\n",
    "            torch.tensor(input_ids,dtype = torch.float, device=self.device),\n",
    "            torch.tensor(target_pos,dtype = torch.float, device=self.device),\n",
    "            torch.tensor(answer,dtype = torch.float, device=self.device),\n",
    "        )\n",
    "\n",
    "        return cur_tensors\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        self.encoding.requires_grad = False\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        return x + self.encoding[:, :x.size(1)].detach().to(device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.h = num_heads # 병렬 attention head 개수\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        # self.d_model = hidden_dim * num_heads ##\n",
    "        self.d_model = hidden_dim ##\n",
    "        \n",
    "        self.Q_fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.K_fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.V_fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.O_fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_dim, 1e-6)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        n_batch = Q.size(0)\n",
    "        residual = Q\n",
    "        \n",
    "        def transform(x, fc):\n",
    "            out = fc(x)\n",
    "            out = out.view(n_batch, -1, self.h, self.head_dim)\n",
    "            out = out.transpose(1, 2)\n",
    "            return out\n",
    "        \n",
    "        Q = transform(Q, self.Q_fc)\n",
    "        K = transform(K, self.K_fc)\n",
    "        V = transform(V, self.V_fc)\n",
    "        d_k = K.shape[-1]\n",
    "        device = torch.device('cuda')\n",
    "        attention_score = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_score = attention_score / torch.sqrt(torch.tensor(d_k, dtype=torch.double, device=device))\n",
    "            \n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e12)\n",
    "        \n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "        out = torch.matmul(attention_score, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(n_batch, -1, self.d_model)\n",
    "        out = self.O_fc(out)\n",
    "        out = self.dropout(out) + residual\n",
    "        return self.layerNorm(out)\n",
    "    \n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size * 4)\n",
    "        self.activation = lambda x: x * 0.5 * (1.0 + torch.erf(x / sqrt(2.0)))\n",
    "        self.fc2 = nn.Linear(self.hidden_size * 4, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerNorm = nn.LayerNorm(self.hidden_size, 1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out) + residual\n",
    "        return out\n",
    "\n",
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim, num_heads, dropout)\n",
    "        self.pointwise_feed_forward = PointWiseFeedForward(hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = self.self_attention(x, x, x, mask)\n",
    "        out = self.pointwise_feed_forward(out)\n",
    "        return out\n",
    "    \n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, max_seq_length, hidden_dim, num_heads, num_blocks, dropout):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, max_seq_length)\n",
    "        self.sas_blocks = nn.ModuleList([SASRecBlock(hidden_dim, num_heads, dropout) for _ in range(num_blocks)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "        self.initializer_range = args.initializer_range\n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def make_pad_mask(self, x, pad_idx=0): ##\n",
    "        max_seq_length = x.size(-1)\n",
    "\n",
    "        row_wise = x.ne(pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        row_wise = row_wise.repeat(1, 1, 1, max_seq_length)\n",
    "\n",
    "        column_wise = x.ne(pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        column_wise = column_wise.repeat(1, 1, max_seq_length, 1)\n",
    "\n",
    "        pad_mask = row_wise & column_wise\n",
    "        pad_mask.requires_grad = False\n",
    "\n",
    "        return pad_mask\n",
    "    \n",
    "    def make_subsequent_mask(self, x, pad_idx=0): ##\n",
    "        max_seq_length = x.size(-1)\n",
    "\n",
    "        attention_shape = (1, max_seq_length, max_seq_length)\n",
    "        subsequent_mask = torch.triu(torch.ones(attention_shape), diagonal=1)\n",
    "        subsequent_mask = (subsequent_mask == pad_idx).unsqueeze(1)\n",
    "        subsequent_mask.requires_grad = False\n",
    "\n",
    "        return subsequent_mask\n",
    "    \n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, input_seq, mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        seqs = input_seq\n",
    "        seqs += self.positional_encoding(seqs)\n",
    "\n",
    "        mask_pad = self.make_pad_mask(mask).to(seqs.device) ## \n",
    "        mask_time = self.make_subsequent_mask(mask).to(seqs.device) ##\n",
    "        mask = mask_pad & mask_time\n",
    "\n",
    "        # print(mask.shape)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for sas_block in self.sas_blocks: \n",
    "            seqs = sas_block(seqs, mask)\n",
    "        output = seqs\n",
    "        return output\n",
    "    \n",
    "def setting_model(args):\n",
    "    model = SASRec(args.max_len, args.d_embed, args.num_heads, args.num_layers, args.dropout_rate).to(args.device)\n",
    "    # 모델의 가중치 파라미터를 Double 형식으로 설정\n",
    "    model.apply(lambda module: setattr(module, 'dtype', torch.double))\n",
    "    return model\n",
    "\n",
    "class Annoy:\n",
    "    def __init__(self, args, annoy_index):\n",
    "        self.n_trees = args.n_trees\n",
    "        self.n = args.n\n",
    "        self.d_embed = args.d_embed\n",
    "        self.annoy_index = annoy_index\n",
    "        \n",
    "    def find_annoy(self, vector):\n",
    "        ann_idx, ann_score = self.annoy_index.get_nns_by_vector(vector, self.n, include_distances=True)\n",
    "        return ann_idx[::-1],ann_score[::-1]\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self,args, key, transformer, annoy_index, count_dict):\n",
    "        self.args = args\n",
    "        self.n_trees = self.args.n_trees\n",
    "        self.n = self.args.n\n",
    "        self.d_embed = self.args.d_embed\n",
    "        self.key = key\n",
    "        self.model = transformer\n",
    "        self.annoy_index = annoy_index\n",
    "        self.device = self.args.device\n",
    "        \n",
    "    def inference(self):\n",
    "        device = torch.device(self.device)\n",
    "        idx = corp2idx[self.key]\n",
    "        inference_dataset = Dataset(args, corp_seq, data_type=\"inference\")[idx]\n",
    "        inference_dataloader = DataLoader(inference_dataset, batch_size= 1)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            corp_id, mask, input_seq, target_pos, _ = inference_dataloader\n",
    "            mask = mask.to(device)\n",
    "            input_seq, target_pos = input_seq.to(device), target_pos.to(device)\n",
    "            output = self.model(input_seq, mask=mask)\n",
    "            output = output[:, -1, :].cpu().data.numpy()\n",
    "        output = output.reshape(-1,1)\n",
    "        return output\n",
    "    \n",
    "    def scoring(self):\n",
    "        self.inference_vector = self.inference()\n",
    "        annoy = Annoy(args, self.annoy_index)\n",
    "        ann_idx,ann_score = annoy.find_annoy(self.inference_vector) # (768, 1)\n",
    "        score_dict = {}\n",
    "        for idx,score in zip(ann_idx,ann_score):\n",
    "            key = df_biz.iloc[idx]['key']\n",
    "            if key not in score_dict:\n",
    "                score_dict[key] = [1,score]\n",
    "            else:\n",
    "                score_dict[key][0] += 1\n",
    "                \n",
    "        return score_dict\n",
    "    \n",
    "    def find_candidates(self):\n",
    "        score_dict = self.scoring()\n",
    "        k = len(score_dict)\n",
    "        # corp_list = np.repeat(df_biz[df_biz['key']==self.key]['회사명'].iloc[0],k)\n",
    "        corp_list = np.repeat(key2corpname[self.key],k)\n",
    "        key_list = np.repeat(self.key, k)\n",
    "        sorted_score = sorted(score_dict.items(), key=lambda x: (x[1][0],x[1][1]), reverse=True)\n",
    "        target_corp_list, target_key_list = [],[]\n",
    "        for target_corp_info in sorted_score:\n",
    "            target_corp_key = target_corp_info[0]\n",
    "            target_corp = key2corpname[target_corp_key]\n",
    "            target_corp_list.append(target_corp)\n",
    "            target_key_list.append(target_corp_key)\n",
    "        df_candidate = pd.DataFrame({'의뢰기업':corp_list,'의뢰기업_key':key_list,'대상기업':target_corp_list,'대상기업_key':target_key_list})\n",
    "        return df_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895ee3e3-b7fc-4788-bc01-19ce14561407",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse()\n",
    "\n",
    "args.seed = 42\n",
    "seed_everything(args)\n",
    "\n",
    "args.server_home_path = '.'\n",
    "args.server_data_path = args.server_home_path + '/data'\n",
    "\n",
    "args.gcp_home_path = ''\n",
    "args.gcp_data_path = args.gcp_home_path\n",
    "args.gcp_imgs_path = args.gcp_data_path + 'imgs/'\n",
    "\n",
    "args.key_name = '/linear-yen-393604-4b0eb55e48b1.json'\n",
    "args.bucket_name = 'exit_bucket'\n",
    "args.patent_name = 'patent_companies.tsv' \n",
    "args.app_number_name = 'number.npy'\n",
    "args.embedding_name = 'embedding.npy'\n",
    "args.common_name = 'biz2common.tsv'\n",
    "\n",
    "args.transformer_path = args.gcp_data_path + 'transformer/'\n",
    "args.transformer_version = 'version1'\n",
    "args.transformer_file_name = f'transformer_{args.transformer_version}.pt'\n",
    "args.d_embed=768\n",
    "args.max_len = 1024\n",
    "args.initializer_range = 0.02\n",
    "args.num_heads = 4\n",
    "args.num_layers = 2\n",
    "args.dropout_rate = 0.2\n",
    "args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "args.ann_path = args.server_data_path + '/ann'\n",
    "args.ann_version = 'version1'\n",
    "args.ann_file_name = f'/ann_{args.ann_version}.annoy'\n",
    "args.n_trees = 20\n",
    "args.n = 100\n",
    "\n",
    "args.valuation_path = args.gcp_data_path + 'valuation/'\n",
    "args.valuation_version = 'version1'\n",
    "args.valuation_file_name = f'valuation_{args.valuation_version}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140bfde9-ab23-4f5d-b02b-bf695284875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.77718949317932\n",
      "72.11422228813171\n",
      "164.63167023658752\n",
      "164.98443794250488\n",
      "261.459237575531\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "args.bucket = init_gcs(args)\n",
    "\n",
    "url = get_url_from_gcs(args, args.gcp_data_path, args.patent_name)\n",
    "response = requests.get(url)\n",
    "df_biz = pd.read_csv(io.BytesIO(response.content), sep = '\\t', parse_dates=['출원일자'])\n",
    "df_biz = df_biz[df_biz['출원일자'].notna()]\n",
    "print(time.time() - start)\n",
    "\n",
    "url = get_url_from_gcs(args, args.gcp_data_path, args.app_number_name)\n",
    "response = requests.get(url)\n",
    "number = np.load(io.BytesIO(response.content), allow_pickle = True)\n",
    "print(time.time() - start)\n",
    "\n",
    "url = get_url_from_gcs(args, args.gcp_data_path, args.embedding_name)\n",
    "response = requests.get(url)\n",
    "embedding = np.load(io.BytesIO(response.content), allow_pickle = True)\n",
    "print(time.time() - start)\n",
    "\n",
    "url = get_url_from_gcs(args, args.gcp_data_path, args.common_name)\n",
    "response = requests.get(url)\n",
    "biz2common = pd.read_csv(io.BytesIO(response.content), sep = '\\t', dtype={'사업자번호':'object','index':'object'})\n",
    "print(time.time() - start)\n",
    "\n",
    "df = make_data_frame(df_biz, number, embedding, biz2common)\n",
    "biz2idx = dict(zip(df['사업자번호'],df['key']))\n",
    "idx2biz = dict(zip(df['key'],df['사업자번호'].apply(lambda x : f'{x[:3]}-{x[3:5]}-{x[5:]}')))\n",
    "df_above, df_under, count_table, count_dict, corp2idx, idx2corp = preprocess_dataset(df, 'train') ## 1개짜리는..?\n",
    "args.num_items = len(count_dict)\n",
    "corp_seq = make_seq_main(args, df_above, count_dict)\n",
    "key2corpname = dict(zip(biz2common['index'], biz2common['회사명']))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892f486-d914-45fb-b822-085fe1213a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30005번 포트로 접속 대기중...\n",
      "('182.218.59.27', 53172) 에서 접속되었습니다.\n",
      "{'transformer_version': 'version1', 'ann_version': 'version1', 'valuation_version': 'version1'}\n",
      "30005번 포트로 접속 대기중...\n",
      "('182.218.59.27', 53176) 에서 접속되었습니다.\n",
      "{'business_number': '125-86-25094'}\n",
      "30005번 포트로 접속 대기중...\n",
      "('182.218.59.27', 53186) 에서 접속되었습니다.\n",
      "{'transformer_version': 'version1', 'ann_version': 'version1', 'valuation_version': 'version1'}\n",
      "30005번 포트로 접속 대기중...\n",
      "('182.218.59.27', 53187) 에서 접속되었습니다.\n",
      "{'business_number': '125-86-25094'}\n",
      "30005번 포트로 접속 대기중...\n"
     ]
    }
   ],
   "source": [
    "# 서버 IP 및 열어줄 포트\n",
    "HOST = ''\n",
    "PORT = 30005\n",
    "PREV = dict()\n",
    "\n",
    "serverSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "serverSock.bind(('', PORT))\n",
    "serverSock.listen(1)\n",
    "\n",
    "# try:\n",
    "while True:\n",
    "    try:\n",
    "        print(f'{PORT}번 포트로 접속 대기중...')\n",
    "        connectionSock, addr = serverSock.accept()\n",
    "\n",
    "        print(str(addr), '에서 접속되었습니다.')\n",
    "\n",
    "        # -------------------------------\n",
    "        recvData = connectionSock.recv(1024)\n",
    "        recvData = json.loads(recvData.decode('utf-8'))\n",
    "        print(recvData)\n",
    "\n",
    "        if 'transformer_version' in recvData.keys():\n",
    "        \n",
    "            args.transformer_version = recvData['transformer_version']\n",
    "            args.ann_version = recvData['ann_version']\n",
    "            args.valuation_version = recvData['valuation_version']\n",
    "\n",
    "            if 'transformer_version' not in PREV.keys() or PREV['transformer_version'] != args.transformer_version:\n",
    "                PREV['transformer_version'] = args.transformer_version\n",
    "                args.transformer_file_name = f'transformer_{args.transformer_version}.pt'\n",
    "\n",
    "                url = get_url_from_gcs(args, args.transformer_path, args.transformer_file_name)\n",
    "                response = requests.get(url)\n",
    "                transformer_path = io.BytesIO(response.content)\n",
    "\n",
    "                transformer = setting_model(args)\n",
    "                transformer.load_state_dict(torch.load(transformer_path))\n",
    "\n",
    "            if 'ann_version' not in PREV.keys() or PREV['ann_version'] != args.ann_version:\n",
    "                PREV['ann_version'] = args.ann_version\n",
    "                args.ann_file_name = f'/ann_{args.ann_version}.annoy'\n",
    "\n",
    "                annoy_index = AnnoyIndex(args.d_embed, 'angular') # batch로 돈다고 가정해야 할듯 함\n",
    "                annoy_index.load(args.server_data_path + args.ann_file_name)\n",
    "\n",
    "            if 'valuation_version' not in PREV.keys() or PREV['valuation_version'] != args.valuation_version:\n",
    "                PREV['valuation_version'] = args.valuation_version\n",
    "                args.valuation_file_name = f'valuation_{args.valuation_version}.csv'\n",
    "\n",
    "                url = get_url_from_gcs(args, args.valuation_path, args.valuation_file_name)\n",
    "                response = requests.get(url)\n",
    "                valuation = pd.read_csv(io.BytesIO(response.content))\n",
    "                valuation['index'] = valuation['index'].astype(str)\n",
    "\n",
    "            sendData = 'Completed'\n",
    "            connectionSock.send(sendData.encode('utf-8'))\n",
    "            \n",
    "\n",
    "        elif 'business_number' in recvData.keys():\n",
    "            args.business_number = biz2idx[recvData['business_number'].replace('-', '')]\n",
    "\n",
    "            inference = Inference(args, args.business_number, transformer, annoy_index, count_dict)\n",
    "            inference_return = inference.find_candidates()\n",
    "\n",
    "            inference_return = inference_return.merge(valuation[['index', 'value']], left_on = '의뢰기업_key', right_on = 'index', how = 'left').drop('index', axis = 1).rename(columns = {'value' : '의뢰기업_value'})\n",
    "            inference_return = inference_return.merge(valuation[['index', 'value']], left_on = '대상기업_key', right_on = 'index', how = 'left').drop('index', axis = 1).rename(columns = {'value' : '대상기업_value'})\n",
    "\n",
    "            inference_return['의뢰기업_biz'] = inference_return['의뢰기업_key'].map(idx2biz)\n",
    "            inference_return['대상기업_biz'] = inference_return['대상기업_key'].map(idx2biz)\n",
    "\n",
    "            inference_return = inference_return[inference_return['의뢰기업_value'] > inference_return['대상기업_value']]\n",
    "            inference_return = inference_return[:10]\n",
    "            \n",
    "            temp_v1 = dict(zip(inference_return['의뢰기업'], inference_return['의뢰기업_value']))\n",
    "            temp_k1 = dict(zip(inference_return['의뢰기업'], inference_return['의뢰기업_key']))\n",
    "            temp_b1 = dict(zip(inference_return['의뢰기업'], inference_return['의뢰기업_biz']))\n",
    "\n",
    "            temp_v2 = dict(zip(inference_return['대상기업'], inference_return['대상기업_value']))\n",
    "            temp_k2 = dict(zip(inference_return['대상기업'], inference_return['대상기업_key']))\n",
    "            temp_b2 = dict(zip(inference_return['대상기업'], inference_return['대상기업_biz']))\n",
    "\n",
    "            result = []\n",
    "            for key in temp_v1.keys():\n",
    "                result.append(key)\n",
    "                result.append(str(temp_v1[key]))\n",
    "                result.append(str(temp_k1[key]))\n",
    "                result.append(str(temp_b1[key]))\n",
    "\n",
    "            for key in temp_v2.keys():\n",
    "                result.append(key)\n",
    "                result.append(str(temp_v2[key]))\n",
    "                result.append(str(temp_k2[key]))\n",
    "                result.append(str(temp_b2[key]))\n",
    "\n",
    "            sendData = ' '.join(result)\n",
    "            connectionSock.send(sendData.encode('utf-8'))\n",
    "\n",
    "        connectionSock.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error :', e)\n",
    "        connectionSock.close()\n",
    "serverSock.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
